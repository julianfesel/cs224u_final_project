
@misc{wang_self-consistency_2023,
	title = {Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2203.11171},
	doi = {10.48550/arXiv.2203.11171},
	abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
	month = mar,
	year = {2023},
	note = {arXiv:2203.11171 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Published at ICLR 2023. V2: added PaLM results; V3: added UL2 results; V4: camera ready version at ICLR 2023},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/UP35LABX/Wang et al. - 2023 - Self-Consistency Improves Chain of Thought Reasoni.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/HHM5VLRE/2203.html:text/html},
}

@misc{yao_tree_2023,
	title = {Tree of {Thoughts}: {Deliberate} {Problem} {Solving} with {Large} {Language} {Models}},
	shorttitle = {Tree of {Thoughts}},
	url = {http://arxiv.org/abs/2305.10601},
	doi = {10.48550/arXiv.2305.10601},
	abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
	month = dec,
	year = {2023},
	note = {arXiv:2305.10601 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: NeurIPS 2023 camera ready version. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/CDZPCWZ5/Yao et al. - 2023 - Tree of Thoughts Deliberate Problem Solving with .pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/3MLBVC9X/2305.html:text/html},
}

@misc{besta_graph_2024,
	title = {Graph of {Thoughts}: {Solving} {Elaborate} {Problems} with {Large} {Language} {Models}},
	shorttitle = {Graph of {Thoughts}},
	url = {http://arxiv.org/abs/2308.09687},
	doi = {10.48550/arXiv.2308.09687},
	abstract = {We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62\% over ToT, while simultaneously reducing costs by {\textgreater}31\%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
	month = feb,
	year = {2024},
	note = {arXiv:2308.09687 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/MCXCJHQY/Besta et al. - 2024 - Graph of Thoughts Solving Elaborate Problems with.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/P5E279Z9/2308.html:text/html},
}

@misc{shinn_reflexion_2023,
	title = {Reflexion: {Language} {Agents} with {Verbal} {Reinforcement} {Learning}},
	shorttitle = {Reflexion},
	url = {http://arxiv.org/abs/2303.11366},
	doi = {10.48550/arXiv.2303.11366},
	abstract = {Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91\% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80\%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Shinn, Noah and Cassano, Federico and Berman, Edward and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
	month = oct,
	year = {2023},
	note = {arXiv:2303.11366 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: v4 contains a few additional experiments},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/VYANNKQ8/Shinn et al. - 2023 - Reflexion Language Agents with Verbal Reinforceme.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/9KATXGLD/2303.html:text/html},
}

@misc{yao_react_2023,
	title = {{ReAct}: {Synergizing} {Reasoning} and {Acting} in {Language} {Models}},
	shorttitle = {{ReAct}},
	url = {http://arxiv.org/abs/2210.03629},
	doi = {10.48550/arXiv.2210.03629},
	abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
	month = mar,
	year = {2023},
	note = {arXiv:2210.03629 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: v3 is the ICLR camera ready version with some typos fixed. Project site with code: https://react-lm.github.io},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/BJ64F7M5/Yao et al. - 2023 - ReAct Synergizing Reasoning and Acting in Languag.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/PZ75R2PQ/2210.html:text/html},
}

@misc{madaan_self-refine_2023,
	title = {Self-{Refine}: {Iterative} {Refinement} with {Self}-{Feedback}},
	shorttitle = {Self-{Refine}},
	url = {http://arxiv.org/abs/2303.17651},
	doi = {10.48550/arXiv.2303.17651},
	abstract = {Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by {\textasciitilde}20\% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
	month = may,
	year = {2023},
	note = {arXiv:2303.17651 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Code, data, and demo at https://selfrefine.info/},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/N9TBSN2U/Madaan et al. - 2023 - Self-Refine Iterative Refinement with Self-Feedba.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/352HKFMU/2303.html:text/html},
}

@misc{kojima_large_2023,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Reasoners}},
	url = {http://arxiv.org/abs/2205.11916},
	doi = {10.48550/arXiv.2205.11916},
	abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	month = jan,
	year = {2023},
	note = {arXiv:2205.11916 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Accepted to NeurIPS2022. Our code is available at https://github.com/kojima-takeshi188/zero\_shot\_cot},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/LFJ4A4MK/Kojima et al. - 2023 - Large Language Models are Zero-Shot Reasoners.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/X8LSNT98/2205.html:text/html},
}

@misc{wang_plan-and-solve_2023,
	title = {Plan-and-{Solve} {Prompting}: {Improving} {Zero}-{Shot} {Chain}-of-{Thought} {Reasoning} by {Large} {Language} {Models}},
	shorttitle = {Plan-and-{Solve} {Prompting}},
	url = {http://arxiv.org/abs/2305.04091},
	doi = {10.48550/arXiv.2305.04091},
	abstract = {Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual effort, Zero-shot-CoT concatenates the target problem statement with "Let's think step by step" as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng},
	month = may,
	year = {2023},
	note = {arXiv:2305.04091 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ACL 2023},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/VHLZNT8D/Wang et al. - 2023 - Plan-and-Solve Prompting Improving Zero-Shot Chai.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/I9DEUGYD/2305.html:text/html},
}

@misc{press_measuring_2023,
	title = {Measuring and {Narrowing} the {Compositionality} {Gap} in {Language} {Models}},
	url = {http://arxiv.org/abs/2210.03350},
	doi = {10.48550/arXiv.2210.03350},
	abstract = {We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A. and Lewis, Mike},
	month = oct,
	year = {2023},
	note = {arXiv:2210.03350 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: To appear at Findings of EMNLP 2023},
	annote = {Comment: To appear at Findings of EMNLP 2023},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/Y8CPMN3C/Press et al. - 2023 - Measuring and Narrowing the Compositionality Gap i.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/URVFSQL8/2210.html:text/html},
}

@misc{phd_complete_2024,
	title = {A {Complete} {Guide} to {LLMs}-based {Autonomous} {Agents} ({Part} {I}):},
	shorttitle = {A {Complete} {Guide} to {LLMs}-based {Autonomous} {Agents} ({Part} {I})},
	url = {https://medium.com/the-modern-scientist/a-complete-guide-to-llms-based-autonomous-agents-part-i-69515c016792},
	abstract = {— — Chain of Thought, Plan and Solve/Execute, Self-Ask, ReAct, Reflexion, Self-Consistency, Tree of Thoughts and Graph of Thoughts},
	language = {en},
	urldate = {2024-03-09},
	journal = {The Modern Scientist},
	author = {PhD, Yule Wang},
	month = feb,
	year = {2024},
	file = {Snapshot:/Users/julianfesel/Zotero/storage/7HUYPUFI/a-complete-guide-to-llms-based-autonomous-agents-part-i-69515c016792.html:text/html},
}

@misc{jiang_kg-agent_2024,
	title = {{KG}-{Agent}: {An} {Efficient} {Autonomous} {Agent} {Framework} for {Complex} {Reasoning} over {Knowledge} {Graph}},
	shorttitle = {{KG}-{Agent}},
	url = {http://arxiv.org/abs/2402.11163},
	doi = {10.48550/arXiv.2402.11163},
	abstract = {In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets. Our code and data will be publicly released.},
	urldate = {2024-03-10},
	publisher = {arXiv},
	author = {Jiang, Jinhao and Zhou, Kun and Zhao, Wayne Xin and Song, Yang and Zhu, Chen and Zhu, Hengshu and Wen, Ji-Rong},
	month = feb,
	year = {2024},
	note = {arXiv:2402.11163 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: work in progress; efficient 7B LLM-based agent},
	annote = {Comment: work in progress; efficient 7B LLM-based agent},
	file = {Jiang et al. - 2024 - KG-Agent An Efficient Autonomous Agent Framework .pdf:/Users/julianfesel/Zotero/storage/ZEX3734Q/Jiang et al. - 2024 - KG-Agent An Efficient Autonomous Agent Framework .pdf:application/pdf},
}

@misc{wang_survey_2023,
	title = {A {Survey} on {Large} {Language} {Model} based {Autonomous} {Agents}},
	url = {http://arxiv.org/abs/2308.11432},
	doi = {10.48550/arXiv.2308.11432},
	abstract = {Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
	month = sep,
	year = {2023},
	note = {arXiv:2308.11432 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 35 pages, 5 figures, 3 tables},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/9DIHRK2A/Wang et al. - 2023 - A Survey on Large Language Model based Autonomous .pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/C7CVGJQ4/2308.html:text/html},
}

@inproceedings{handler_taxonomy_2023,
	address = {Rome, Italy},
	title = {A {Taxonomy} for {Autonomous} {LLM}-{Powered} {Multi}-{Agent} {Architectures}:},
	isbn = {978-989-758-671-2},
	shorttitle = {A {Taxonomy} for {Autonomous} {LLM}-{Powered} {Multi}-{Agent} {Architectures}},
	url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0012239100003598},
	doi = {10.5220/0012239100003598},
	language = {en},
	urldate = {2024-03-09},
	booktitle = {Proceedings of the 15th {International} {Joint} {Conference} on {Knowledge} {Discovery}, {Knowledge} {Engineering} and {Knowledge} {Management}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Händler, Thorsten},
	year = {2023},
	pages = {85--98},
	file = {Händler - 2023 - A Taxonomy for Autonomous LLM-Powered Multi-Agent .pdf:/Users/julianfesel/Zotero/storage/3H7CQ8ZJ/Händler - 2023 - A Taxonomy for Autonomous LLM-Powered Multi-Agent .pdf:application/pdf},
}

@misc{zeng_agenttuning_2023,
	title = {{AgentTuning}: {Enabling} {Generalized} {Agent} {Abilities} for {LLMs}},
	shorttitle = {{AgentTuning}},
	url = {http://arxiv.org/abs/2310.12823},
	doi = {10.48550/arXiv.2310.12823},
	abstract = {Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hybrid instruction-tuning strategy by combining AgentInstruct with open-source instructions from general domains. AgentTuning is used to instruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show that AgentTuning enables LLMs' agent capabilities without compromising general abilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent tasks, demonstrating generalized agent capabilities. We open source the AgentInstruct and AgentLM-7B, 13B, and 70B models at https://github.com/THUDM/AgentTuning, serving open and powerful alternatives to commercial LLMs for agent tasks.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Zeng, Aohan and Liu, Mingdao and Lu, Rui and Wang, Bowen and Liu, Xiao and Dong, Yuxiao and Tang, Jie},
	month = oct,
	year = {2023},
	note = {arXiv:2310.12823 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 31 pages},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/IYEKS27M/Zeng et al. - 2023 - AgentTuning Enabling Generalized Agent Abilities .pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/P3WQKNZ9/2310.html:text/html},
}

@misc{yang_auto-gpt_2023,
	title = {Auto-{GPT} for {Online} {Decision} {Making}: {Benchmarks} and {Additional} {Opinions}},
	shorttitle = {Auto-{GPT} for {Online} {Decision} {Making}},
	url = {http://arxiv.org/abs/2306.02224},
	doi = {10.48550/arXiv.2306.02224},
	abstract = {Auto-GPT is an autonomous agent that leverages recent advancements in adapting Large Language Models (LLMs) for decision-making tasks. While there has been a growing interest in Auto-GPT stypled agents, questions remain regarding the effectiveness and flexibility of Auto-GPT in solving real-world decision-making tasks. Its limited capability for real-world engagement and the absence of benchmarks contribute to these uncertainties. In this paper, we present a comprehensive benchmark study of Auto-GPT styled agents in decision-making tasks that simulate real-world scenarios. Our aim is to gain deeper insights into this problem and understand the adaptability of GPT-based agents. We compare the performance of popular LLMs such as GPT-4, GPT-3.5, Claude, and Vicuna in Auto-GPT styled decision-making tasks. Furthermore, we introduce the Additional Opinions algorithm, an easy and effective method that incorporates supervised/imitation-based learners into the Auto-GPT scheme. This approach enables lightweight supervised learning without requiring fine-tuning of the foundational LLMs. We demonstrate through careful baseline comparisons and ablation studies that the Additional Opinions algorithm significantly enhances performance in online decision-making benchmarks, including WebShop and ALFWorld.},
	urldate = {2024-03-12},
	publisher = {arXiv},
	author = {Yang, Hui and Yue, Sifu and He, Yunzhong},
	month = jun,
	year = {2023},
	note = {arXiv:2306.02224 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/JQVYAR8K/Yang et al. - 2023 - Auto-GPT for Online Decision Making Benchmarks an.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/6SYJR8MX/2306.html:text/html},
}

@misc{wu_autogen_2023,
	title = {{AutoGen}: {Enabling} {Next}-{Gen} {LLM} {Applications} via {Multi}-{Agent} {Conversation}},
	shorttitle = {{AutoGen}},
	url = {http://arxiv.org/abs/2308.08155},
	doi = {10.48550/arXiv.2308.08155},
	abstract = {AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and Awadallah, Ahmed Hassan and White, Ryen W. and Burger, Doug and Wang, Chi},
	month = oct,
	year = {2023},
	note = {arXiv:2308.08155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 43 pages (10 pages for the main text, 3 pages for references, and 30 pages for appendices)},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/KKG5I7LL/Wu et al. - 2023 - AutoGen Enabling Next-Gen LLM Applications via Mu.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/ERC553W2/2308.html:text/html},
}

@article{rasheed_autonomous_2023,
	title = {Autonomous {Agents} in {Software} {Development}: {A} {Vision} {Paper}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Autonomous {Agents} in {Software} {Development}},
	url = {https://arxiv.org/abs/2311.18440},
	doi = {10.48550/ARXIV.2311.18440},
	abstract = {Large Language Models (LLM) and Generative Pre-trained Transformers (GPT), are reshaping the field of Software Engineering (SE). They enable innovative methods for executing many software engineering tasks, including automated code generation, debugging, maintenance, etc. However, only a limited number of existing works have thoroughly explored the potential of GPT agents in SE. This vision paper inquires about the role of GPT-based agents in SE. Our vision is to leverage the capabilities of multiple GPT agents to contribute to SE tasks and to propose an initial road map for the future work. We argue that multiple GPT agents can perform creative and demanding tasks far beyond coding and debugging. GPT agents can also do project planning, requirements engineering, and software design. These can be done through high-level descriptions given by the human developer. We have shown in our initial experimental analysis for simple software (e.g., Snake Game, Tic-Tac-Toe, Notepad) that multiple GPT agents can produce high-quality code and document it carefully; We argue that it shows a promise of unforeseen efficiency and will dramatically reduce lead-times. To this end, we intend to expand our efforts to understand how we can scale these autonomous capabilities further. We provide the video link below as a demonstration of our initial work 1.},
	language = {en},
	urldate = {2024-03-11},
	author = {Rasheed, Zeeshan and Waseem, Muhammad and Kemell, Kai-Kristian and Xiaofeng, Wang and Duc, Anh Nguyen and Systä, Kari and Abrahamsson, Pekka},
	year = {2023},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {FOS: Computer and information sciences, Software Engineering (cs.SE)},
	annote = {Other
5 pages, 1 figure},
	file = {Rasheed et al. - 2023 - Autonomous Agents in Software Development A Visio.pdf:/Users/julianfesel/Zotero/storage/DB8DZMY6/Rasheed et al. - 2023 - Autonomous Agents in Software Development A Visio.pdf:application/pdf},
}

@misc{handler_balancing_2023,
	title = {Balancing {Autonomy} and {Alignment}: {A} {Multi}-{Dimensional} {Taxonomy} for {Autonomous} {LLM}-powered {Multi}-{Agent} {Architectures}},
	shorttitle = {Balancing {Autonomy} and {Alignment}},
	url = {http://arxiv.org/abs/2310.03659},
	doi = {10.48550/arXiv.2310.03659},
	abstract = {Large language models (LLMs) have revolutionized the field of artificial intelligence, endowing it with sophisticated language understanding and generation capabilities. However, when faced with more complex and interconnected tasks that demand a profound and iterative thought process, LLMs reveal their inherent limitations. Autonomous LLM-powered multi-agent systems represent a strategic response to these challenges. Such systems strive for autonomously tackling user-prompted goals by decomposing them into manageable tasks and orchestrating their execution and result synthesis through a collective of specialized intelligent agents. Equipped with LLM-powered reasoning capabilities, these agents harness the cognitive synergy of collaborating with their peers, enhanced by leveraging contextual resources such as tools and datasets. While these architectures hold promising potential in amplifying AI capabilities, striking the right balance between different levels of autonomy and alignment remains the crucial challenge for their effective operation. This paper proposes a comprehensive multi-dimensional taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems balance the dynamic interplay between autonomy and alignment across various aspects inherent to architectural viewpoints such as goal-driven task management, agent composition, multi-agent collaboration, and context interaction. It also includes a domain-ontology model specifying fundamental architectural concepts. Our taxonomy aims to empower researchers, engineers, and AI practitioners to systematically analyze the architectural dynamics and balancing strategies employed by these increasingly prevalent AI systems. The exploratory taxonomic classification of selected representative LLM-powered multi-agent systems illustrates its practical utility and reveals potential for future research and development.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Händler, Thorsten},
	month = oct,
	year = {2023},
	note = {arXiv:2310.03659 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/T3QNTX8K/Händler - 2023 - Balancing Autonomy and Alignment A Multi-Dimensio.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/VC9DIFBQ/2310.html:text/html},
}

@misc{liu_bolaa_2023,
	title = {{BOLAA}: {Benchmarking} and {Orchestrating} {LLM}-augmented {Autonomous} {Agents}},
	shorttitle = {{BOLAA}},
	url = {http://arxiv.org/abs/2308.05960},
	doi = {10.48550/arXiv.2308.05960},
	abstract = {The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, {\textbackslash}textit\{i.e.\} BOLAA, where a controller manages the communication among multiple agents. We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs. Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both. We release our implementation code of LAAs to the public at {\textbackslash}url\{https://github.com/salesforce/BOLAA\}.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Liu, Zhiwei and Yao, Weiran and Zhang, Jianguo and Xue, Le and Heinecke, Shelby and Murthy, Rithesh and Feng, Yihao and Chen, Zeyuan and Niebles, Juan Carlos and Arpit, Devansh and Xu, Ran and Mui, Phil and Wang, Huan and Xiong, Caiming and Savarese, Silvio},
	month = aug,
	year = {2023},
	note = {arXiv:2308.05960 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Preprint},
}

@misc{lu_building_2023,
	title = {Building the {Future} of {Responsible} {AI}: {A} {Reference} {Architecture} for {Designing} {Large} {Language} {Model} based {Agents}},
	shorttitle = {Building the {Future} of {Responsible} {AI}},
	url = {http://arxiv.org/abs/2311.13148},
	doi = {10.48550/arXiv.2311.13148},
	abstract = {Large language models (LLMs) have been widely recognised as transformative artificial generative intelligence (AGI) technologies due to their capabilities to understand and generate content, including plans with reasoning capabilities. Foundation model based agents derive their autonomy from the capabilities of foundation models, which enable them to autonomously break down a given goal into a set of manageable tasks and orchestrate task execution to meet the goal. Despite the huge efforts put into building foundation model based autonomous agents, the architecture design of the agents has not yet been systematically explored. Also, while there are significant benefits of using autonomous agents for planning and execution, there are serious considerations regarding responsible AI related software quality attributes, such as security and accountability. Therefore, this paper presents a pattern-oriented reference architecture that serves as architecture design guidance and enables responsible-AI-by-design when designing foundation model based autonomous agents. We evaluate the completeness and utility of the proposed reference architecture by mapping it to the architecture of two real-world agents.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Lu, Qinghua and Zhu, Liming and Xu, Xiwei and Xing, Zhenchang and Harrer, Stefan and Whittle, Jon},
	month = nov,
	year = {2023},
	note = {arXiv:2311.13148 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/Y9JFMISY/Lu et al. - 2023 - Building the Future of Responsible AI A Reference.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/FRRYAUYZ/2311.html:text/html},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2024-03-17},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/FCRTPE38/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/DZKMZ6WE/2201.html:text/html},
}

@misc{nijkamp_codegen2_2023,
	title = {{CodeGen2}: {Lessons} for {Training} {LLMs} on {Programming} and {Natural} {Languages}},
	shorttitle = {{CodeGen2}},
	url = {http://arxiv.org/abs/2305.02309},
	doi = {10.48550/arXiv.2305.02309},
	abstract = {Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly. In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a "free lunch" hypothesis. For data distributions, the effect of a mixture distribution and multi-epoch training of programming and natural languages on model performance is explored. We conduct a comprehensive series of empirical experiments on 1B LLMs, for which failures and successes of this exploration are distilled into five lessons. We will provide a final recipe for training and release CodeGen2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source: https://github.com/salesforce/CodeGen.},
	urldate = {2024-03-14},
	publisher = {arXiv},
	author = {Nijkamp, Erik and Hayashi, Hiroaki and Xiong, Caiming and Savarese, Silvio and Zhou, Yingbo},
	month = jul,
	year = {2023},
	note = {arXiv:2305.02309 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/PEPTH547/Nijkamp et al. - 2023 - CodeGen2 Lessons for Training LLMs on Programming.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/XL7EEELK/2305.html:text/html},
}

@misc{rasheed_codepori_2024,
	title = {{CodePori}: {Large} {Scale} {Model} for {Autonomous} {Software} {Development} by {Using} {Multi}-{Agents}},
	shorttitle = {{CodePori}},
	url = {http://arxiv.org/abs/2402.01411},
	doi = {10.48550/arXiv.2402.01411},
	abstract = {Large Language Models (LLMs) and Generative Pre-trained Transformers (GPTs) are reshaping the field of Software Engineering (SE). Existing LLM-based multi-agent systems have successfully resolved simple dialogue tasks. However, the potential of LLMs for more complex tasks, such as automated code generation for large and complex projects, have been explored in only a few existing works. This paper introduces CodePori, a novel model designed to automate code generation for extensive and complex software projects based on natural language prompts. We employ LLM-based multi-AI agents to handle creative and challenging tasks in autonomous software development. Each agent engages with a specific task, including system design, code development, code review, code verification, and test engineering. We show in the paper that CodePori is able to generate running code for large-scale projects, completing the entire software development process in minutes rather than hours, and at a cost of a few dollars. It identifies and mitigates potential security vulnerabilities and corrects errors while maintaining a solid code performance level. We also conducted an evaluation of CodePori against existing solutions using HumanEval and the Massively Multitask Benchmark for Python (MBPP) benchmark. The results indicate that CodePori improves upon the benchmarks in terms of code accuracy, efficiency, and overall performance. For example, CodePori improves the pass@1 metric on HumanEval to 87.5\% and on MBPP to 86.5\%, representing a clear improvement over the existing models. We also assessed CodePori's performance through practitioner evaluations, with 91\% expressing satisfaction with the model's performance.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Rasheed, Zeeshan and Waseem, Muhammad and Saari, Mika and Systä, Kari and Abrahamsson, Pekka},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01411 [cs]},
	keywords = {Computer Science - Software Engineering},
	annote = {Comment: 10 pages and 3 figures},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/2WRWNGFD/Rasheed et al. - 2024 - CodePori Large Scale Model for Autonomous Softwar.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/86WSCHHX/2402.html:text/html},
}

@misc{sumers_cognitive_2023,
	title = {Cognitive {Architectures} for {Language} {Agents}},
	url = {http://arxiv.org/abs/2309.02427},
	doi = {10.48550/arXiv.2309.02427},
	abstract = {Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within the broader history of AI and outlines a path towards language-based general intelligence.},
	urldate = {2024-03-10},
	publisher = {arXiv},
	author = {Sumers, Theodore R. and Yao, Shunyu and Narasimhan, Karthik and Griffiths, Thomas L.},
	month = sep,
	year = {2023},
	note = {arXiv:2309.02427 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Symbolic Computation},
	annote = {Comment: v2 enriched actionable insights and discussions, and polished abstract and introduction. 18 pages of main content, 12 pages of references, 5 figures. The first two authors contributed equally, order decided by coin flip. A CoALA-based repo of recent work on language agents: https://github.com/ysymyth/awesome-language-agents},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/SIYGWCDL/Sumers et al. - 2023 - Cognitive Architectures for Language Agents.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/7PRA4HU6/2309.html:text/html},
}

@article{geva_did_2021,
	title = {\textit{{Did} {Aristotle} {Use} a {Laptop}?} {A} {Question} {Answering} {Benchmark} with {Implicit} {Reasoning} {Strategies}},
	volume = {9},
	issn = {2307-387X},
	shorttitle = {{\textless}i{\textgreater}{Did} {Aristotle} {Use} a {Laptop}?},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00370/100680/Did-Aristotle-Use-a-Laptop-A-Question-Answering},
	doi = {10.1162/tacl_a_00370},
	abstract = {Abstract
            A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87\%) on this task, while our best baseline reaches an accuracy of ∼ 66\%.},
	language = {en},
	urldate = {2024-03-12},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},
	month = apr,
	year = {2021},
	pages = {346--361},
	annote = {[TLDR] This work introduces StrategyQA, a question answering benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy, and proposes a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts.},
	file = {Full Text:/Users/julianfesel/Zotero/storage/3DR24TAJ/Geva et al. - 2021 - Did Aristotle Use a Laptop A Question Answ.pdf:application/pdf},
}

@misc{wang_executable_2024,
	title = {Executable {Code} {Actions} {Elicit} {Better} {LLM} {Agents}},
	url = {http://arxiv.org/abs/2402.01030},
	doi = {10.48550/arXiv.2402.01030},
	abstract = {Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20\% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.},
	urldate = {2024-03-10},
	publisher = {arXiv},
	author = {Wang, Xingyao and Chen, Yangyi and Yuan, Lifan and Zhang, Yizhe and Li, Yunzhu and Peng, Hao and Ji, Heng},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01030 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Code, data, model, and demo are available at https://github.com/xingyaoww/code-act},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/S6ASWVK7/Wang et al. - 2024 - Executable Code Actions Elicit Better LLM Agents.pdf:application/pdf},
}

@misc{chen_fireact_2023,
	title = {{FireAct}: {Toward} {Language} {Agent} {Fine}-tuning},
	shorttitle = {{FireAct}},
	url = {http://arxiv.org/abs/2310.05915},
	doi = {10.48550/arXiv.2310.05915},
	abstract = {Recent efforts have augmented language models (LMs) with external tools or environments, leading to the development of language agents that can reason and act. However, most of these agents rely on few-shot prompting techniques with off-the-shelf LMs. In this paper, we investigate and argue for the overlooked direction of fine-tuning LMs to obtain language agents. Using a setup of question answering (QA) with a Google search API, we explore a variety of base LMs, prompting methods, fine-tuning data, and QA tasks, and find language agents are consistently improved after fine-tuning their backbone LMs. For example, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4 leads to a 77\% HotpotQA performance increase. Furthermore, we propose FireAct, a novel approach to fine-tuning LMs with trajectories from multiple tasks and prompting methods, and show having more diverse fine-tuning data can further improve agents. Along with other findings regarding scaling effects, robustness, generalization, efficiency and cost, our work establishes comprehensive benefits of fine-tuning LMs for agents, and provides an initial set of experimental designs, insights, as well as open questions toward language agent fine-tuning.},
	urldate = {2024-03-10},
	publisher = {arXiv},
	author = {Chen, Baian and Shu, Chang and Shareghi, Ehsan and Collier, Nigel and Narasimhan, Karthik and Yao, Shunyu},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05915 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Code, data, and models are available at https://fireact-agent.github.io},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/CUG66NAB/Chen et al. - 2023 - FireAct Toward Language Agent Fine-tuning.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/VIXTKLUZ/2310.html:text/html},
}

@misc{lyu_gitagent_2023,
	title = {{GitAgent}: {Facilitating} {Autonomous} {Agent} with {GitHub} by {Tool} {Extension}},
	shorttitle = {{GitAgent}},
	url = {http://arxiv.org/abs/2312.17294},
	doi = {10.48550/arXiv.2312.17294},
	abstract = {While Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated exceptional proficiency in natural language processing, their efficacy in addressing complex, multifaceted tasks remains limited. A growing area of research focuses on LLM-based agents equipped with external tools capable of performing diverse tasks. However, existing LLM-based agents only support a limited set of tools which is unable to cover a diverse range of user queries, especially for those involving expertise domains. It remains a challenge for LLM-based agents to extend their tools autonomously when confronted with various user queries. As GitHub has hosted a multitude of repositories which can be seen as a good resource for tools, a promising solution is that LLM-based agents can autonomously integrate the repositories in GitHub according to the user queries to extend their tool set. In this paper, we introduce GitAgent, an agent capable of achieving the autonomous tool extension from GitHub. GitAgent follows a four-phase procedure to incorporate repositories and it can learn human experience by resorting to GitHub Issues/PRs to solve problems encountered during the procedure. Experimental evaluation involving 30 user queries demonstrates GitAgent's effectiveness, achieving a 69.4\% success rate on average.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Lyu, Bohan and Cong, Xin and Yu, Heyang and Yang, Pan and Qin, Yujia and Ye, Yining and Lu, Yaxi and Zhang, Zhong and Yan, Yukun and Lin, Yankai and Liu, Zhiyuan and Sun, Maosong},
	month = dec,
	year = {2023},
	note = {arXiv:2312.17294 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/FZAXRLZI/Lyu et al. - 2023 - GitAgent Facilitating Autonomous Agent with GitHu.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/P7VH3DJW/2312.html:text/html},
}

@misc{patil_gorilla_2023,
	title = {Gorilla: {Large} {Language} {Model} {Connected} with {Massive} {APIs}},
	shorttitle = {Gorilla},
	url = {http://arxiv.org/abs/2305.15334},
	doi = {10.48550/arXiv.2305.15334},
	abstract = {Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu},
	urldate = {2024-03-10},
	publisher = {arXiv},
	author = {Patil, Shishir G. and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E.},
	month = may,
	year = {2023},
	note = {arXiv:2305.15334 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/XFBI9B9X/Patil et al. - 2023 - Gorilla Large Language Model Connected with Massi.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/T6M49363/2305.html:text/html},
}

@misc{yang_hotpotqa_2018,
	title = {{HotpotQA}: {A} {Dataset} for {Diverse}, {Explainable} {Multi}-hop {Question} {Answering}},
	shorttitle = {{HotpotQA}},
	url = {http://arxiv.org/abs/1809.09600},
	abstract = {Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HOTPOTQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require ﬁnding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems’ ability to extract relevant facts and perform necessary comparison. We show that HOTPOTQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.},
	language = {en},
	urldate = {2024-03-12},
	publisher = {arXiv},
	author = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
	month = sep,
	year = {2018},
	note = {arXiv:1809.09600 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: EMNLP 2018 long paper. The first three authors contribute equally. Data, code, and blog posts available at https://hotpotqa.github.io/},
	file = {Yang et al. - 2018 - HotpotQA A Dataset for Diverse, Explainable Multi.pdf:/Users/julianfesel/Zotero/storage/TAHWJM5G/Yang et al. - 2018 - HotpotQA A Dataset for Diverse, Explainable Multi.pdf:application/pdf},
}

@misc{shen_hugginggpt_2023,
	title = {{HuggingGPT}: {Solving} {AI} {Tasks} with {ChatGPT} and its {Friends} in {Hugging} {Face}},
	shorttitle = {{HuggingGPT}},
	url = {http://arxiv.org/abs/2303.17580},
	doi = {10.48550/arXiv.2303.17580},
	abstract = {Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
	month = dec,
	year = {2023},
	note = {arXiv:2303.17580 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/KAVAGRTG/Shen et al. - 2023 - HuggingGPT Solving AI Tasks with ChatGPT and its .pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/5EJ4UQLH/2303.html:text/html},
}

@misc{weng_llm_2023,
	title = {{LLM} {Powered} {Autonomous} {Agents}},
	url = {https://lilianweng.github.io/posts/2023-06-23-agent/},
	abstract = {Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.
Agent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:},
	language = {en},
	urldate = {2024-03-12},
	author = {Weng, Lilian},
	month = jun,
	year = {2023},
	note = {Section: posts},
	file = {Snapshot:/Users/julianfesel/Zotero/storage/89IHJAMF/2023-06-23-agent.html:text/html},
}

@inproceedings{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {https://openreview.net/forum?id=nZeVKeeFYf9},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by a factor of 10,000 and the GPU memory requirement by a factor of 3. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	language = {en},
	urldate = {2024-03-11},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	file = {Full Text PDF:/Users/julianfesel/Zotero/storage/5CGTRLP7/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf},
}

@inproceedings{barkur_magenta_2024,
	title = {Magenta: {Metrics} and {Evaluation} {Framework} for {Generative} {Agents} based on {LLMs}},
	volume = {119},
	isbn = {978-1-958651-95-7},
	shorttitle = {Magenta},
	url = {https://openaccess.cms-conferences.org/publications/book/978-1-958651-95-7/article/978-1-958651-95-7_14},
	doi = {10.54941/ahfe1004478},
	abstract = {Large Language Models (LLMs) have emerged as a driving force in the field of Natural Language Processing (NLP) with applications spanning various domains, including the development of Autonomous Generative Agents. Generative Agents are computational software programs designed to believably simulate human behavior by harnessing the capabilities of large language models. Through repetitive prompts against the large language model, these agents operate based on a system architecture consisting of memory streams, reflection, and planning, allowing them to store experiences, learn from them, and translate insights into high-level action plans to interact with their environment. This paper discusses the current landscape of language models and autonomous agents, their advantages and challenges, and the current state of evaluation, and proposes an innovative evaluation benchmark designed to provide a holistic perspective on their performance. Additionally, we see the impact of fine-tuning such an LLM, evaluate using our benchmark, and then propose a framework for evaluation of both the agents and their underlying LLMs. The existing frameworks for evaluating LLMs and autonomous agents focus on single tasks and are limited in capturing their capabilities. We outline the methodology for evaluating autonomous agents' performance in responding to single and multi-step prompts. The process consists of three key stages: Preparation of the data, Preparation of the Gold Answers, and Evaluations. We use the meticulously crafted 20 unique prompts to challenge the agents, covering simple and complex questions. Using GPT-4, a state-of-the-art model, we generate the initial responses, which undergo rigorous verification to produce gold answers, indicating correctness and revealing the minimum steps required for task completion. Our evaluation framework relies on two critical metrics: the effort metrics, quantifying the steps taken by autonomous agents, and the success rate, measuring their accuracy in achieving task objectives and also keeping track of hallucinations of the model. We conduct experiments with ten different models, representing the current landscape of natural language processing models, presenting each with 20 unique prompts. Their responses are meticulously compared to our gold answers and gold steps (optimal number of steps) to generate the evaluation metrics. Similarly, a fine-tuned model is also evaluated with ten different questions, which test the agent's decision-making process by selecting the correct tool and then the ability of the model to reach the correct conclusion to the question asked by the user in this process.This comprehensive approach ensures a thorough assessment of autonomous agents' capabilities. It demonstrates the utility of these metrics, revealing how they can shed light on the strengths and weaknesses of various autonomous agents. As a step toward standardization, we propose transforming the evaluation process of LLMs into an automated framework that accommodates all types of language models, agents, and LLM-based applications. Such an approach promises to establish a unified and comprehensive evaluation methodology, empowering users to make informed decisions when selecting, fine-tuning, and assessing the accuracy of underlying language models and their applications for different domains.In summary, this paper contributes to the ongoing research on evaluating LLMs and autonomous agents by introducing a novel benchmark and proposing a framework, focusing on evaluating the language models while keeping different knowledge domains in mind. Our framework will enhance our understanding of these technologies and serve as a valuable resource for researchers, engineers, and practitioners working in the ever-evolving landscape of NLP and autonomous systems.},
	language = {eng},
	urldate = {2024-03-10},
	booktitle = {Intelligent {Human} {Systems} {Integration} ({IHSI} 2024): {Integrating} {People} and {Intelligent} {Systems}},
	publisher = {AHFE Open Acces},
	author = {Barkur, Sudarshan Kamath and Sitapara, Pratik and Leuschner, Sven and Schacht, Sigurd},
	year = {2024},
	note = {ISSN: 27710718
Issue: 119},
	file = {Barkur et al. - 2024 - Magenta Metrics and Evaluation Framework for Gene.pdf:/Users/julianfesel/Zotero/storage/5MWPT8GM/Barkur et al. - 2024 - Magenta Metrics and Evaluation Framework for Gene.pdf:application/pdf},
}

@misc{hendrycks_measuring_2021,
	title = {Measuring {Massive} {Multitask} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2009.03300},
	doi = {10.48550/arXiv.2009.03300},
	abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
	urldate = {2024-03-12},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	month = jan,
	year = {2021},
	note = {arXiv:2009.03300 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Computers and Society},
	annote = {Comment: ICLR 2021; the test and code is available at https://github.com/hendrycks/test},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/U59LF6AT/Hendrycks et al. - 2021 - Measuring Massive Multitask Language Understanding.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/ULEBCDP4/2009.html:text/html},
}

@article{liu_reason_2023,
	title = {Reason for {Future}, {Act} for {Now}: {A} {Principled} {Architecture} for {Autonomous} {LLM} {Agents}},
	shorttitle = {Reason for {Future}, {Act} for {Now}},
	url = {https://openreview.net/forum?id=5aHmaMFJns},
	abstract = {Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call "reason for future, act for now" (\${\textbackslash}texttt\{RAFA\}\$). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon ("reason for future"). At each step, the LLM agent takes the initial action of the planned trajectory ("act for now"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state. The key idea is to cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt LLMs to form an updated posterior of the unknown environment from the memory buffer (learning) and generate an optimal trajectory for multiple future steps that maximizes a value function (planning). The learning and planning subroutines are performed in an "in-context" manner to emulate the actor-critic update for MDPs. Our theoretical analysis proves that the novel combination of long-term reasoning and short-term acting achieves a \${\textbackslash}sqrt\{T\}\$ regret. In particular, the regret bound highlights an intriguing interplay between the prior knowledge obtained through pretraining and the uncertainty reduction achieved by reasoning and acting. Our empirical validation shows that it outperforms various existing frameworks and achieves nearly perfect scores on a few benchmarks. By incorporating "classical" MDP techniques, \${\textbackslash}texttt\{RAFA\}\$ introduces the first autonomous LLM agent with provable regret guarantees.},
	language = {en},
	urldate = {2024-03-09},
	author = {Liu, Zhihan and Hu, Hao and Zhang, Shenao and Guo, Hongyi and Ke, Shuqi and Liu, Boyi and Wang, Zhaoran},
	month = oct,
	year = {2023},
	file = {Full Text PDF:/Users/julianfesel/Zotero/storage/THC3B7KS/Liu et al. - 2023 - Reason for Future, Act for Now A Principled Archi.pdf:application/pdf},
}

@misc{wang_self-instruct_2023,
	title = {Self-{Instruct}: {Aligning} {Language} {Models} with {Self}-{Generated} {Instructions}},
	shorttitle = {Self-{Instruct}},
	url = {http://arxiv.org/abs/2212.10560},
	doi = {10.48550/arXiv.2212.10560},
	abstract = {Large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https://github.com/yizhongw/self-instruct.},
	urldate = {2024-03-13},
	publisher = {arXiv},
	author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
	month = may,
	year = {2023},
	note = {arXiv:2212.10560 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: ACL 2023 camera ready, 23 pages, 9 figures, 11 tables},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/HGLMVRMD/Wang et al. - 2023 - Self-Instruct Aligning Language Models with Self-.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/234GUZBA/2212.html:text/html},
}

@misc{li_starcoder_2023,
	title = {{StarCoder}: may the source be with you!},
	shorttitle = {{StarCoder}},
	url = {http://arxiv.org/abs/2305.06161},
	doi = {10.48550/arXiv.2305.06161},
	abstract = {The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40{\textbackslash}\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.},
	urldate = {2024-03-14},
	publisher = {arXiv},
	author = {Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and Liu, Qian and Zheltonozhskii, Evgenii and Zhuo, Terry Yue and Wang, Thomas and Dehaene, Olivier and Davaadorj, Mishig and Lamy-Poirier, Joel and Monteiro, João and Shliazhko, Oleh and Gontier, Nicolas and Meade, Nicholas and Zebaze, Armel and Yee, Ming-Ho and Umapathi, Logesh Kumar and Zhu, Jian and Lipkin, Benjamin and Oblokulov, Muhtasham and Wang, Zhiruo and Murthy, Rudra and Stillerman, Jason and Patel, Siva Sankalp and Abulkhanov, Dmitry and Zocca, Marco and Dey, Manan and Zhang, Zhihan and Fahmy, Nour and Bhattacharyya, Urvashi and Yu, Wenhao and Singh, Swayam and Luccioni, Sasha and Villegas, Paulo and Kunakov, Maxim and Zhdanov, Fedor and Romero, Manuel and Lee, Tony and Timor, Nadav and Ding, Jennifer and Schlesinger, Claire and Schoelkopf, Hailey and Ebert, Jan and Dao, Tri and Mishra, Mayank and Gu, Alex and Robinson, Jennifer and Anderson, Carolyn Jane and Dolan-Gavitt, Brendan and Contractor, Danish and Reddy, Siva and Fried, Daniel and Bahdanau, Dzmitry and Jernite, Yacine and Ferrandis, Carlos Muñoz and Hughes, Sean and Wolf, Thomas and Guha, Arjun and von Werra, Leandro and de Vries, Harm},
	month = dec,
	year = {2023},
	note = {arXiv:2305.06161 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/G7LBLB42/Li et al. - 2023 - StarCoder may the source be with you!.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/D9BVTHGE/2305.html:text/html},
}

@misc{lin_swiftsage_2023,
	title = {{SwiftSage}: {A} {Generative} {Agent} with {Fast} and {Slow} {Thinking} for {Complex} {Interactive} {Tasks}},
	shorttitle = {{SwiftSage}},
	url = {http://arxiv.org/abs/2305.17390},
	doi = {10.48550/arXiv.2305.17390},
	abstract = {We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex interactive tasks.},
	urldate = {2024-03-10},
	publisher = {arXiv},
	author = {Lin, Bill Yuchen and Fu, Yicheng and Yang, Karina and Brahman, Faeze and Huang, Shiyu and Bhagavatula, Chandra and Ammanabrolu, Prithviraj and Choi, Yejin and Ren, Xiang},
	month = dec,
	year = {2023},
	note = {arXiv:2305.17390 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Multiagent Systems, Computer Science - Robotics},
	annote = {Comment: Accepted to NeurIPS 2023 (spotlight). Project website: https://swiftsage.github.io},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/LMPZXJAL/Lin et al. - 2023 - SwiftSage A Generative Agent with Fast and Slow T.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/Q9QJZUUA/2305.html:text/html},
}

@misc{shen_taskbench_2023,
	title = {{TaskBench}: {Benchmarking} {Large} {Language} {Models} for {Task} {Automation}},
	shorttitle = {{TaskBench}},
	url = {http://arxiv.org/abs/2311.18760},
	doi = {10.48550/arXiv.2311.18760},
	abstract = {Recently, the incredible progress of large language models (LLMs) has ignited the spark of task automation, which decomposes the complex tasks described by user instructions into sub-tasks, and invokes external tools to execute them, and plays a central role in autonomous agents. However, there lacks a systematic and standardized benchmark to foster the development of LLMs in task automation. To this end, we introduce TaskBench to evaluate the capability of LLMs in task automation. Specifically, task automation can be formulated into three critical stages: task decomposition, tool invocation, and parameter prediction to fulfill user intent. This complexity makes data collection and evaluation more challenging compared to common NLP tasks. To generate high-quality evaluation datasets, we introduce the concept of Tool Graph to represent the decomposed tasks in user intent, and adopt a back-instruct method to simulate user instruction and annotations. Furthermore, we propose TaskEval to evaluate the capability of LLMs from different aspects, including task decomposition, tool invocation, and parameter prediction. Experimental results demonstrate that TaskBench can effectively reflects the capability of LLMs in task automation. Benefiting from the mixture of automated data construction and human verification, TaskBench achieves a high consistency compared to the human evaluation, which can be utilized as a comprehensive and faithful benchmark for LLM-based autonomous agents.},
	urldate = {2024-03-12},
	publisher = {arXiv},
	author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Zhang, Wenqi and Ren, Kan and Yuan, Siyu and Lu, Weiming and Li, Dongsheng and Zhuang, Yueting},
	month = dec,
	year = {2023},
	note = {arXiv:2311.18760 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/A6QETVJB/Shen et al. - 2023 - TaskBench Benchmarking Large Language Models for .pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/MK4DJ9TJ/2311.html:text/html},
}

@misc{qiao_taskweaver_2023,
	title = {{TaskWeaver}: {A} {Code}-{First} {Agent} {Framework}},
	shorttitle = {{TaskWeaver}},
	url = {http://arxiv.org/abs/2311.17541},
	doi = {10.48550/arXiv.2311.17541},
	abstract = {Large Language Models (LLMs) have shown impressive abilities in natural language understanding and generation, leading to their use in applications such as chatbots and virtual assistants. However, existing LLM frameworks face limitations in handling domain-specific data analytics tasks with rich data structures. Moreover, they struggle with flexibility to meet diverse user requirements. To address these issues, TaskWeaver is proposed as a code-first framework for building LLM-powered autonomous agents. It converts user requests into executable code and treats user-defined plugins as callable functions. TaskWeaver provides support for rich data structures, flexible plugin usage, and dynamic plugin selection, and leverages LLM coding capabilities for complex logic. It also incorporates domain-specific knowledge through examples and ensures the secure execution of generated code. TaskWeaver offers a powerful and flexible framework for creating intelligent conversational agents that can handle complex tasks and adapt to domain-specific scenarios. The code is open-sourced at https://github.com/microsoft/TaskWeaver/.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Qiao, Bo and Li, Liqun and Zhang, Xu and He, Shilin and Kang, Yu and Zhang, Chaoyun and Yang, Fangkai and Dong, Hang and Zhang, Jue and Wang, Lu and Ma, Minghua and Zhao, Pu and Qin, Si and Qin, Xiaoting and Du, Chao and Xu, Yong and Lin, Qingwei and Rajmohan, Saravan and Zhang, Dongmei},
	month = dec,
	year = {2023},
	note = {arXiv:2311.17541 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/AEEV8J52/Qiao et al. - 2023 - TaskWeaver A Code-First Agent Framework.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/KATHNST5/2311.html:text/html},
}

@misc{xi_rise_2023,
	title = {The {Rise} and {Potential} of {Large} {Language} {Model} {Based} {Agents}: {A} {Survey}},
	shorttitle = {The {Rise} and {Potential} of {Large} {Language} {Model} {Based} {Agents}},
	url = {http://arxiv.org/abs/2309.07864},
	doi = {10.48550/arXiv.2309.07864},
	abstract = {For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and Zheng, Rui and Fan, Xiaoran and Wang, Xiao and Xiong, Limao and Zhou, Yuhao and Wang, Weiran and Jiang, Changhao and Zou, Yicheng and Liu, Xiangyang and Yin, Zhangyue and Dou, Shihan and Weng, Rongxiang and Cheng, Wensen and Zhang, Qi and Qin, Wenjuan and Zheng, Yongyan and Qiu, Xipeng and Huang, Xuanjing and Gui, Tao},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07864 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 86 pages, 12 figures},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/ILTRYP4L/Xi et al. - 2023 - The Rise and Potential of Large Language Model Bas.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/VZQFVTNU/2309.html:text/html},
}

@misc{schick_toolformer_2023,
	title = {Toolformer: {Language} {Models} {Can} {Teach} {Themselves} to {Use} {Tools}},
	shorttitle = {Toolformer},
	url = {http://arxiv.org/abs/2302.04761},
	doi = {10.48550/arXiv.2302.04761},
	abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q{\textbackslash}\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
	urldate = {2024-03-12},
	publisher = {arXiv},
	author = {Schick, Timo and Dwivedi-Yu, Jane and Dessì, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04761 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/QHHKLTWT/Schick et al. - 2023 - Toolformer Language Models Can Teach Themselves t.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/CND7PWQV/2302.html:text/html},
}

@misc{qin_toolllm_2023,
	title = {{ToolLLM}: {Facilitating} {Large} {Language} {Models} to {Master} 16000+ {Real}-world {APIs}},
	shorttitle = {{ToolLLM}},
	url = {http://arxiv.org/abs/2307.16789},
	doi = {10.48550/arXiv.2307.16789},
	abstract = {Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.},
	urldate = {2024-03-10},
	publisher = {arXiv},
	author = {Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and Zhao, Sihan and Hong, Lauren and Tian, Runchu and Xie, Ruobing and Zhou, Jie and Gerstein, Mark and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
	month = oct,
	year = {2023},
	note = {arXiv:2307.16789 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/UAT4EASQ/Qin et al. - 2023 - ToolLLM Facilitating Large Language Models to Mas.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/WMCEQHQU/2307.html:text/html},
}

@misc{huang_understanding_2024,
	title = {Understanding the planning of {LLM} agents: {A} survey},
	shorttitle = {Understanding the planning of {LLM} agents},
	url = {http://arxiv.org/abs/2402.02716},
	doi = {10.48550/arXiv.2402.02716},
	abstract = {As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Huang, Xu and Liu, Weiwen and Chen, Xiaolong and Wang, Xingmei and Wang, Hao and Lian, Defu and Wang, Yasheng and Tang, Ruiming and Chen, Enhong},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02716 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 9 pages, 2 tables, 2 figures},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/K9B8SWG6/Huang et al. - 2024 - Understanding the planning of LLM agents A survey.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/396NT3XH/2402.html:text/html},
}

@misc{nakano_webgpt_2022,
	title = {{WebGPT}: {Browser}-assisted question-answering with human feedback},
	shorttitle = {{WebGPT}},
	url = {http://arxiv.org/abs/2112.09332},
	doi = {10.48550/arXiv.2112.09332},
	abstract = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
	urldate = {2024-03-10},
	publisher = {arXiv},
	author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
	month = jun,
	year = {2022},
	note = {arXiv:2112.09332 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 32 pages},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/NM3DH4SR/Nakano et al. - 2022 - WebGPT Browser-assisted question-answering with h.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/GTKV2A5Q/2112.html:text/html},
}

@misc{yao_webshop_2023,
	title = {{WebShop}: {Towards} {Scalable} {Real}-{World} {Web} {Interaction} with {Grounded} {Language} {Agents}},
	shorttitle = {{WebShop}},
	url = {http://arxiv.org/abs/2207.01206},
	doi = {10.48550/arXiv.2207.01206},
	abstract = {Existing benchmarks for grounding language in interactive environments either lack real-world linguistic elements, or prove difficult to scale up due to substantial human involvement in the collection of data or feedback signals. To bridge this gap, we develop WebShop -- a simulated e-commerce website environment with \$1.18\$ million real-world products and \$12,087\$ crowd-sourced text instructions. Given a text instruction specifying a product requirement, an agent needs to navigate multiple types of webpages and issue diverse actions to find, customize, and purchase an item. WebShop provides several challenges for language grounding including understanding compositional instructions, query (re-)formulation, comprehending and acting on noisy text in webpages, and performing strategic exploration. We collect over \$1,600\$ human demonstrations for the task, and train and evaluate a diverse range of agents using reinforcement learning, imitation learning, and pre-trained image and language models. Our best model achieves a task success rate of \$29{\textbackslash}\%\$, which outperforms rule-based heuristics (\$9.6{\textbackslash}\%\$) but is far lower than human expert performance (\$59{\textbackslash}\%\$). We also analyze agent and human trajectories and ablate various model components to provide insights for developing future agents with stronger language understanding and decision making abilities. Finally, we show that agents trained on WebShop exhibit non-trivial sim-to-real transfer when evaluated on amazon.com and ebay.com, indicating the potential value of WebShop in developing practical web-based agents that can operate in the wild.},
	urldate = {2024-03-10},
	publisher = {arXiv},
	author = {Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
	month = feb,
	year = {2023},
	note = {arXiv:2207.01206 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Project page with code, data, demos: https://webshop-pnlp.github.io. v3 is NeurIPS camera ready version. v4 fixes the choice oracle result as per https://github.com/princeton-nlp/WebShop/issues/15},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/ZYTXPPVG/Yao et al. - 2023 - WebShop Towards Scalable Real-World Web Interacti.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/4VP97TNZ/2207.html:text/html},
}

@article{yao_survey_2024,
	title = {A survey on {Large} {Language} {Model} ({LLM}) security and privacy: {The} {Good}, {The} {Bad}, and {The} {Ugly}},
	issn = {2667-2952},
	shorttitle = {A survey on {Large} {Language} {Model} ({LLM}) security and privacy},
	url = {https://www.sciencedirect.com/science/article/pii/S266729522400014X},
	doi = {10.1016/j.hcc.2024.100211},
	abstract = {Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into “The Good” (beneficial LLM applications), “The Bad” (offensive applications), and “The Ugly” (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs’ potential to both bolster and jeopardize cybersecurity.},
	urldate = {2024-03-18},
	journal = {High-Confidence Computing},
	author = {Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue},
	month = mar,
	year = {2024},
	keywords = {ChatGPT, Large Language Model (LLM), LLM attacks, LLM privacy, LLM security, LLM vulnerabilities},
	pages = {100211},
	file = {ScienceDirect Snapshot:/Users/julianfesel/Zotero/storage/8BSQRB56/S266729522400014X.html:text/html;Submitted Version:/Users/julianfesel/Zotero/storage/WUCGR6L2/Yao et al. - 2024 - A survey on Large Language Model (LLM) security an.pdf:application/pdf},
}

@article{kakisim_deep_2024,
	title = {A deep learning approach based on multi-view consensus for {SQL} injection detection},
	issn = {1615-5270},
	url = {https://doi.org/10.1007/s10207-023-00791-y},
	doi = {10.1007/s10207-023-00791-y},
	abstract = {SQL injection (SQLi) attacks are one of the oldest and most serious security threats, consistently ranking among the top ten critical web security risks. Traditional defense mechanisms against SQL injection predominantly use blacklists to disallow common injection characters or terms. However, the major challenge for these systems is to create a comprehensive list of potential SQLi characters, terms, and multi-terms that encompass various types of SQLi attacks (time-based, error-based, etc.), taking into account various SQL datasets (such as MySQL, Oracle, and NoSQL). Recently, some research studies have concentrated on feature learning from SQL queries by applying some well-known deep architectures to detect SQLi attacks. Motivated by a similar objective, this research introduces a novel deep learning-based SQLi detection system named “Bidirectional LSTM-CNN based on Multi-View Consensus” (MVC-BiCNN). The proposed method implements a pre-processing step that generates multiple views from SQL data by semantically encoding SQL statements into their corresponding SQL tags. By utilizing two different main layers, which are bidirectional long short-term memory (LSTM) and convolutional neural network (CNN), the proposed method learns a joint latent space from multi-view representations. In the detection phase, the proposed method yields separate predictions for each representation and assesses whether the query constitutes an SQLi attack based on a consensus function’s output. Moreover, Interpretable Model-Agnostic Annotations (LIME), one of the methods of Explainable Artificial Intelligence (XAI), is employed for the purpose of interpreting the model’s results and analyzing the SQL injection (SQLi) inputs. The experimental results demonstrate that MVC-BiCNN outperforms the baseline methods, yielding 99.96\% detection rate.},
	language = {en},
	urldate = {2024-03-18},
	journal = {International Journal of Information Security},
	author = {Kakisim, Arzu Gorgulu},
	month = jan,
	year = {2024},
	keywords = {Deep learning, Code injection, Information security, SQL injection, XAI},
	file = {Full Text PDF:/Users/julianfesel/Zotero/storage/Q78J7AHW/Kakisim - 2024 - A deep learning approach based on multi-view conse.pdf:application/pdf},
}

@article{kakisim_deep_2024-1,
	title = {A deep learning approach based on multi-view consensus for {SQL} injection detection},
	issn = {1615-5270},
	url = {https://doi.org/10.1007/s10207-023-00791-y},
	doi = {10.1007/s10207-023-00791-y},
	abstract = {SQL injection (SQLi) attacks are one of the oldest and most serious security threats, consistently ranking among the top ten critical web security risks. Traditional defense mechanisms against SQL injection predominantly use blacklists to disallow common injection characters or terms. However, the major challenge for these systems is to create a comprehensive list of potential SQLi characters, terms, and multi-terms that encompass various types of SQLi attacks (time-based, error-based, etc.), taking into account various SQL datasets (such as MySQL, Oracle, and NoSQL). Recently, some research studies have concentrated on feature learning from SQL queries by applying some well-known deep architectures to detect SQLi attacks. Motivated by a similar objective, this research introduces a novel deep learning-based SQLi detection system named “Bidirectional LSTM-CNN based on Multi-View Consensus” (MVC-BiCNN). The proposed method implements a pre-processing step that generates multiple views from SQL data by semantically encoding SQL statements into their corresponding SQL tags. By utilizing two different main layers, which are bidirectional long short-term memory (LSTM) and convolutional neural network (CNN), the proposed method learns a joint latent space from multi-view representations. In the detection phase, the proposed method yields separate predictions for each representation and assesses whether the query constitutes an SQLi attack based on a consensus function’s output. Moreover, Interpretable Model-Agnostic Annotations (LIME), one of the methods of Explainable Artificial Intelligence (XAI), is employed for the purpose of interpreting the model’s results and analyzing the SQL injection (SQLi) inputs. The experimental results demonstrate that MVC-BiCNN outperforms the baseline methods, yielding 99.96\% detection rate.},
	language = {en},
	urldate = {2024-03-18},
	journal = {International Journal of Information Security},
	author = {Kakisim, Arzu Gorgulu},
	month = jan,
	year = {2024},
	keywords = {Deep learning, Code injection, Information security, SQL injection, XAI},
}

@misc{chen_learning_2023,
	title = {Learning {To} {Teach} {Large} {Language} {Models} {Logical} {Reasoning}},
	url = {http://arxiv.org/abs/2310.09158},
	doi = {10.48550/arXiv.2310.09158},
	abstract = {Large language models (LLMs) have gained enormous attention from both academia and industry, due to their exceptional ability in language generation and extremely powerful generalization. However, current LLMs still output unreliable content in practical reasoning tasks due to their inherent issues (e.g., hallucination). To better disentangle this problem, in this paper, we conduct an in-depth investigation to systematically explore the capability of LLMs in logical reasoning. More in detail, we first investigate the deficiency of LLMs in logical reasoning on different tasks, including event relation extraction and deductive reasoning. Our study demonstrates that LLMs are not good reasoners in solving tasks with rigorous reasoning and will produce counterfactual answers, which require us to iteratively refine. Therefore, we comprehensively explore different strategies to endow LLMs with logical reasoning ability, and thus enable them to generate more logically consistent answers across different scenarios. Based on our approach, we also contribute a synthesized dataset (LLM-LR) involving multi-hop reasoning for evaluation and pre-training. Extensive quantitative and qualitative analyses on different tasks also validate the effectiveness and necessity of teaching LLMs with logic and provide insights for solving practical tasks with LLMs in future work.},
	urldate = {2024-03-19},
	publisher = {arXiv},
	author = {Chen, Meiqi and Ma, Yubo and Song, Kaitao and Cao, Yixin and Zhang, Yan and Li, Dongsheng},
	month = oct,
	year = {2023},
	note = {arXiv:2310.09158 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/NWAP9XWT/Chen et al. - 2023 - Learning To Teach Large Language Models Logical Re.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/KLRU7MVW/2310.html:text/html},
}

@misc{dettmers_qlora_2023,
	title = {{QLoRA}: {Efficient} {Finetuning} of {Quantized} {LLMs}},
	shorttitle = {{QLoRA}},
	url = {http://arxiv.org/abs/2305.14314},
	doi = {10.48550/arXiv.2305.14314},
	abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
	urldate = {2024-03-20},
	publisher = {arXiv},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	month = may,
	year = {2023},
	note = {arXiv:2305.14314 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Extended NeurIPS submission},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/VY5WYJN4/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/J47JK8ST/2305.html:text/html},
}

@misc{mobarekati_fine-tuning_2024,
	title = {Fine-{Tuning} {Mistral} 7b in {Google} {Colab} with {QLoRA} (complete guide)},
	url = {https://medium.com/@codersama/fine-tuning-mistral-7b-in-google-colab-with-qlora-complete-guide-60e12d437cca},
	abstract = {The resulting fine-tuned model with synthetic data will outperform Openai’s GPT-4 on our domain specific benchmark.},
	language = {en},
	urldate = {2024-03-21},
	journal = {Medium},
	author = {Mobarekati, Ali},
	month = feb,
	year = {2024},
	file = {Snapshot:/Users/julianfesel/Zotero/storage/ATXHBL5A/fine-tuning-mistral-7b-in-google-colab-with-qlora-complete-guide-60e12d437cca.html:text/html},
}

@misc{noauthor_making_nodate,
	title = {Making {LLMs} even more accessible with bitsandbytes, 4-bit quantization and {QLoRA}},
	url = {https://huggingface.co/blog/4bit-transformers-bitsandbytes},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-03-21},
	file = {Snapshot:/Users/julianfesel/Zotero/storage/KLGCW3Z6/4bit-transformers-bitsandbytes.html:text/html},
}

@misc{kim_cot_2023,
	title = {The {CoT} {Collection}: {Improving} {Zero}-shot and {Few}-shot {Learning} of {Language} {Models} via {Chain}-of-{Thought} {Fine}-{Tuning}},
	shorttitle = {The {CoT} {Collection}},
	url = {http://arxiv.org/abs/2305.14045},
	doi = {10.48550/arXiv.2305.14045},
	abstract = {Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal, we first introduce a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection (including only 9 CoT tasks) with additional 1.84 million rationales across 1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B \& 11B) with CoT Collection enables smaller LMs to have better CoT capabilities on unseen tasks. On the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of +4.34\% (Flan-T5 3B) and +2.60\% (Flan-T5 11B), in terms of zero-shot task accuracy. Furthermore, we show that instruction tuning with CoT Collection allows LMs to possess stronger few-shot learning capabilities on 4 domain-specific tasks, resulting in an improvement of +2.24\% (Flan-T5 3B) and +2.37\% (Flan-T5 11B), even outperforming ChatGPT utilizing demonstrations until the max length by a +13.98\% margin. Our code, the CoT Collection data, and model checkpoints are publicly available.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Kim, Seungone and Joo, Se June and Kim, Doyoung and Jang, Joel and Ye, Seonghyeon and Shin, Jamin and Seo, Minjoon},
	month = oct,
	year = {2023},
	note = {arXiv:2305.14045 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: EMNLP 2023 (Main Conference)},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/PDE8SJ4D/Kim et al. - 2023 - The CoT Collection Improving Zero-shot and Few-sh.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/84L9ZJ4B/2305.html:text/html},
}

@misc{chai_xcot_2024,
	title = {{xCoT}: {Cross}-lingual {Instruction} {Tuning} for {Cross}-lingual {Chain}-of-{Thought} {Reasoning}},
	shorttitle = {{xCoT}},
	url = {http://arxiv.org/abs/2401.07037},
	doi = {10.48550/arXiv.2401.07037},
	abstract = {Chain-of-thought (CoT) has emerged as a powerful technique to elicit reasoning in large language models and improve a variety of downstream tasks. CoT mainly demonstrates excellent performance in English, but its usage in low-resource languages is constrained due to poor language generalization. To bridge the gap among different languages, we propose a cross-lingual instruction fine-tuning framework (xCOT) to transfer knowledge from high-resource languages to low-resource languages. Specifically, the multilingual instruction training data (xCOT-INSTRUCT) is created to encourage the semantic alignment of multiple languages. We introduce cross-lingual in-context few-shot learning (xICL)) to accelerate multilingual agreement in instruction tuning, where some fragments of source languages in examples are randomly substituted by their counterpart translations of target languages. During multilingual instruction tuning, we adopt the randomly online CoT strategy to enhance the multilingual reasoning ability of the large language model by first translating the query to another language and then answering in English. To further facilitate the language transfer, we leverage the high-resource CoT to supervise the training of low-resource languages with cross-lingual distillation. Experimental results on previous benchmarks demonstrate the superior performance of xCoT in reducing the gap among different languages, highlighting its potential to reduce the cross-lingual gap.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Chai, Linzheng and Yang, Jian and Sun, Tao and Guo, Hongcheng and Liu, Jiaheng and Wang, Bing and Liang, Xiannian and Bai, Jiaqi and Li, Tongliang and Peng, Qiyao and Li, Zhoujun},
	month = jan,
	year = {2024},
	note = {arXiv:2401.07037 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 11 pages},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/UQZQ35MD/Chai et al. - 2024 - xCoT Cross-lingual Instruction Tuning for Cross-l.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/LG447KHE/2401.html:text/html},
}

@inproceedings{phan_training_2023,
	title = {Training {Chain}-of-{Thought} via {Latent}-{Variable} {Inference}},
	url = {https://openreview.net/forum?id=a147pIS2Co},
	abstract = {Large language models (LLMs) solve problems more accurately and interpretably when instructed to work out the answer step by step using a "chain-of-thought" (CoT) prompt. One can also improve LLMs' performance on a specific task by supervised fine-tuning, i.e., by using gradient ascent on some tunable parameters to maximize the average log-likelihood of correct answers from a labeled training set. Naively combining CoT with supervised tuning requires supervision not just of the correct answers, but also of detailed rationales that lead to those answers; these rationales are expensive to produce by hand. Instead, we propose a fine-tuning strategy that tries to maximize the {\textbackslash}emph\{marginal\} log-likelihood of generating a correct answer using CoT prompting, approximately averaging over all possible rationales. The core challenge is sampling from the posterior over rationales conditioned on the correct answer; we address it using a simple Markov-chain Monte Carlo (MCMC) expectation-maximization (EM) algorithm inspired by the self-taught reasoner (STaR), memoized wake-sleep, Markovian score climbing, and persistent contrastive divergence. This algorithm also admits a novel control-variate technique that drives the variance of our gradient estimates to zero as the model improves. Applying our technique to GSM8K and the tasks in BIG-Bench Hard, we find that this MCMC-EM fine-tuning technique typically improves the model's accuracy on held-out examples more than STaR or prompt-tuning with or without CoT.},
	language = {en},
	urldate = {2024-03-22},
	author = {Phan, Du and Hoffman, Matthew Douglas and Dohan, David and Douglas, Sholto and Le, Tuan Anh and Parisi, Aaron T. and Sountsov, Pavel and Sutton, Charles and Vikram, Sharad and Saurous, Rif A.},
	month = nov,
	year = {2023},
	file = {Full Text PDF:/Users/julianfesel/Zotero/storage/2EBRXYJL/Phan et al. - 2023 - Training Chain-of-Thought via Latent-Variable Infe.pdf:application/pdf},
}

@misc{labonne_fine-tune_2024,
	title = {Fine-tune a {Mistral}-7b model with {Direct} {Preference} {Optimization}},
	url = {https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac},
	abstract = {Boost the performance of your supervised fine-tuned models},
	language = {en},
	urldate = {2024-03-22},
	journal = {Medium},
	author = {Labonne, Maxime},
	month = jan,
	year = {2024},
	file = {Snapshot:/Users/julianfesel/Zotero/storage/ENXW4M9R/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac.html:text/html},
}

@misc{jiang_mistral_2023,
	title = {Mistral {7B}},
	url = {http://arxiv.org/abs/2310.06825},
	doi = {10.48550/arXiv.2310.06825},
	abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06825 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Models and code are available at https://mistral.ai/news/announcing-mistral-7b/},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/MDPBCP57/Jiang et al. - 2023 - Mistral 7B.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/98QGV7N6/2310.html:text/html},
}

@misc{wu_recogs_2024,
	title = {{ReCOGS}: {How} {Incidental} {Details} of a {Logical} {Form} {Overshadow} an {Evaluation} of {Semantic} {Interpretation}},
	shorttitle = {{ReCOGS}},
	url = {http://arxiv.org/abs/2303.13716},
	doi = {10.48550/arXiv.2303.13716},
	abstract = {Compositional generalization benchmarks for semantic parsing seek to assess whether models can accurately compute meanings for novel sentences, but operationalize this in terms of logical form (LF) prediction. This raises the concern that semantically irrelevant details of the chosen LFs could shape model performance. We argue that this concern is realized for the COGS benchmark. COGS poses generalization splits that appear impossible for present-day models, which could be taken as an indictment of those models. However, we show that the negative results trace to incidental features of COGS LFs. Converting these LFs to semantically equivalent ones and factoring out capabilities unrelated to semantic interpretation, we find that even baseline models get traction. A recent variable-free translation of COGS LFs suggests similar conclusions, but we observe this format is not semantically equivalent; it is incapable of accurately representing some COGS meanings. These findings inform our proposal for ReCOGS, a modified version of COGS that comes closer to assessing the target semantic capabilities while remaining very challenging. Overall, our results reaffirm the importance of compositional generalization and careful benchmark task design.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Wu, Zhengxuan and Manning, Christopher D. and Potts, Christopher},
	month = jan,
	year = {2024},
	note = {arXiv:2303.13716 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: TACL 2023},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/4NAYE2IA/Wu et al. - 2024 - ReCOGS How Incidental Details of a Logical Form O.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/3P4MRQ6R/2303.html:text/html},
}

@misc{kim_cogs_2020,
	title = {{COGS}: {A} {Compositional} {Generalization} {Challenge} {Based} on {Semantic} {Interpretation}},
	shorttitle = {{COGS}},
	url = {http://arxiv.org/abs/2010.05465},
	doi = {10.48550/arXiv.2010.05465},
	abstract = {Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96--99\%), but generalization accuracy was substantially lower (16--35\%) and showed high sensitivity to random seed (\${\textbackslash}pm\$6--8\%). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.},
	urldate = {2024-03-28},
	publisher = {arXiv},
	author = {Kim, Najoung and Linzen, Tal},
	month = oct,
	year = {2020},
	note = {arXiv:2010.05465 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to EMNLP 2020},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/T879UNRB/Kim and Linzen - 2020 - COGS A Compositional Generalization Challenge Bas.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/5GUHMP7A/2010.html:text/html},
}

@misc{openai_gpt-4_2024,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2024-03-28},
	publisher = {arXiv},
	author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	month = mar,
	year = {2024},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 100 pages; updated authors list; fixed author names and added citation},
	file = {arXiv Fulltext PDF:/Users/julianfesel/Zotero/storage/WWPHVTMU/OpenAI et al. - 2024 - GPT-4 Technical Report.pdf:application/pdf;arXiv.org Snapshot:/Users/julianfesel/Zotero/storage/CZZVM67E/2303.html:text/html},
}

@misc{unsloth_unslothmistral-7b-instruct-v02-bnb-4bit_2024,
	title = {unsloth/mistral-7b-instruct-v0.2-bnb-4bit · {Hugging} {Face}},
	url = {https://huggingface.co/unsloth/mistral-7b-instruct-v0.2-bnb-4bit},
	urldate = {2024-03-28},
	author = {Unsloth},
	month = mar,
	year = {2024},
	file = {Snapshot:/Users/julianfesel/Zotero/storage/HN3CQLA5/mistral-7b-instruct-v0.html:text/html},
}

@inproceedings{wang_maven-ere_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{MAVEN}-{ERE}: {A} {Unified} {Large}-scale {Dataset} for {Event} {Coreference}, {Temporal}, {Causal}, and {Subevent} {Relation} {Extraction}},
	shorttitle = {{MAVEN}-{ERE}},
	url = {https://aclanthology.org/2022.emnlp-main.60},
	doi = {10.18653/v1/2022.emnlp-main.60},
	abstract = {The diverse relationships among real-world events, including coreference, temporal, causal, and subevent relations, are fundamental to understanding natural languages. However, two drawbacks of existing datasets limit event relation extraction (ERE) tasks: (1) Small scale. Due to the annotation complexity, the data scale of existing datasets is limited, which cannot well train and evaluate data-hungry models. (2) Absence of unified annotation. Different types of event relations naturally interact with each other, but existing datasets only cover limited relation types at once, which prevents models from taking full advantage of relation interactions. To address these issues, we construct a unified large-scale human-annotated ERE dataset MAVEN-ERE with improved annotation schemes. It contains 103,193 event coreference chains, 1,216,217 temporal relations, 57,992 causal relations, and 15,841 subevent relations, which is larger than existing datasets of all the ERE tasks by at least an order of magnitude. Experiments show that ERE on MAVEN-ERE is quite challenging, and considering relation interactions with joint learning can improve performances. The dataset and source codes can be obtained from https://github.com/THU-KEG/MAVEN-ERE.},
	urldate = {2024-04-07},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Xiaozhi and Chen, Yulin and Ding, Ning and Peng, Hao and Wang, Zimu and Lin, Yankai and Han, Xu and Hou, Lei and Li, Juanzi and Liu, Zhiyuan and Li, Peng and Zhou, Jie},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {926--941},
	file = {Full Text PDF:/Users/julianfesel/Zotero/storage/QK2PQYXB/Wang et al. - 2022 - MAVEN-ERE A Unified Large-scale Dataset for Event.pdf:application/pdf},
}

@inproceedings{tafjord_proofwriter_2021,
	address = {Online},
	title = {{ProofWriter}: {Generating} {Implications}, {Proofs}, and {Abductive} {Statements} over {Natural} {Language}},
	shorttitle = {{ProofWriter}},
	url = {https://aclanthology.org/2021.findings-acl.317},
	doi = {10.18653/v1/2021.findings-acl.317},
	urldate = {2024-04-07},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Tafjord, Oyvind and Dalvi, Bhavana and Clark, Peter},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {3621--3634},
	file = {Full Text PDF:/Users/julianfesel/Zotero/storage/QHXPSKIJ/Tafjord et al. - 2021 - ProofWriter Generating Implications, Proofs, and .pdf:application/pdf},
}
